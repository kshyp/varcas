# Complete Optimization Progression

**Date**: 2026-02-11  
**Model**: TheBloke/Llama-2-7B-AWQ (4-bit AWQ)  
**GPU**: Tesla T4  
**Workload**: Chat Medium (20 RPS, ~50 input tokens, ~150 output tokens)

---

## Executive Summary

This document presents the complete optimization journey from original configuration through easy wins to medium-term optimizations.

### Results Overview

| Phase | Configuration | Token Throughput | TPOT | Latency p50 | Status |
|-------|--------------|------------------|------|-------------|--------|
| **Original** | Baseline | 168.8 tok/s | 231.7 ms | 26,013 ms | Starting point |
| **Easy Wins** | +batch=8 | **206.8 tok/s** | **34.5 ms** | **16,981 ms** | âœ… **RECOMMENDED** |
| **Medium-Term** | +chunked | 197.9 tok/s | 37.4 ms | 18,308 ms | âŒ Not beneficial |

**Key Achievement**: +22% throughput, -85% decode latency with Easy Wins

---

## Phase 1: Original Baseline

### Configuration
```bash
python -m vllm.entrypoints.openai.api_server \
  --model TheBloke/Llama-2-7B-AWQ \
  --dtype half \
  --max-model-len 2048 \
  --gpu-memory-utilization 0.90 \
  --quantization awq \
  --enforce-eager \
  --port 8000
```

### Performance
- Token Throughput: 168.8 tok/s
- TPOT (p50): 231.7 ms/token
- Latency (p50): 26,013 ms
- TTFT (p50): 669 ms

### Roofline Analysis
- Decode AI: ~3 FLOP/Byte (memory-bound)
- GPU Utilization: ~1.5% (severely underutilized)
- Bottleneck: Memory bandwidth

---

## Phase 2: Easy Wins âœ…

### Configuration
```bash
python -m vllm.entrypoints.openai.api_server \
  --model TheBloke/Llama-2-7B-AWQ \
  --dtype half \
  --max-model-len 2048 \
  --max-num-seqs 8 \          # â† ADDED
  --gpu-memory-utilization 0.90 \
  --quantization awq \
  --enforce-eager \
  --port 8000
```

### Optimizations Applied
| Optimization | Expected | Actual | Status |
|--------------|----------|--------|--------|
| Batch size (8) | 3-5x throughput | +22% throughput | âœ… |
| CUDA Graphs | 10-20% latency | N/A (AWQ incompatibility) | âš ï¸ |
| Flash Attention | Baseline | Already enabled | âœ… |

### Performance
- Token Throughput: **206.8 tok/s** (+22%)
- TPOT (p50): **34.5 ms/token** (-85%)
- Latency (p50): **16,981 ms** (-35%)
- TTFT (p50): 13,650 ms (+1940%)

### Key Improvements
```
Decode Speed (TPOT):
Before: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 231.7 ms/token
After:  [â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘]  34.5 ms/token
                              6.7x faster!

Token Throughput:
Before: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 168.8 tok/s
After:  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘] 206.8 tok/s
                              +22% improvement
```

### Trade-offs
- âœ… Much faster decode (6.7x)
- âœ… Higher throughput (+22%)
- âœ… Lower total latency (-35%)
- âš ï¸ Higher TTFT (+1940%) - wait longer, but process faster

---

## Phase 3: Medium-Term Optimizations âŒ

### Attempted Optimizations

| Optimization | Expected | Applied | Result | Reason |
|--------------|----------|---------|--------|--------|
| Chunked Prefill | Better interleaving | âœ… | -4% throughput | Not beneficial for short inputs |
| FP8 KV Cache | 30% memory reduction | âŒ | N/A | T4 doesn't support FP8 |
| Speculative Decoding | 2-3x speedup | âŒ | N/A | No draft model available |
| Prefix Caching | KV reuse | âŒ | N/A | AWQ compatibility issues |

### Configuration
```bash
python -m vllm.entrypoints.openai.api_server \
  --model TheBloke/Llama-2-7B-AWQ \
  --dtype half \
  --max-model-len 2048 \
  --max-num-seqs 8 \
  --gpu-memory-utilization 0.90 \
  --quantization awq \
  --enforce-eager \
  --port 8000 \
  --enable-chunked-prefill      # â† ADDED
```

### Performance vs Baseline
- Token Throughput: 197.9 tok/s (-4.3% vs Easy Wins)
- TPOT (p50): 37.4 ms/token (+8.3%)
- Latency (p50): 18,308 ms (+7.8%)

### Why It Didn't Help

Chunked prefill is designed for:
- Long input sequences (1000+ tokens)
- Mixed workloads with varying lengths
- Reducing TTFT for large prefills

**This workload characteristics:**
- Short inputs (~50 tokens)
- Prefill already fast (compute-bound)
- Chunking adds scheduling overhead
- Result: 4-8% performance degradation

---

## Visual Comparison

### Throughput Progression
```
Token Throughput (tok/s)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Original:  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 168.8
Easy Wins: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 206.8  âœ“
Medium:    [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 197.9  âœ—
            0        50       100      150      200      250
```

### Decode Speed Progression
```
Time Per Token (ms) - Lower is Better
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Original:  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 231.7
Easy Wins: [â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘]  34.5  âœ“
Medium:    [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘]  37.4  âœ—
            0        50       100      150      200      250
```

### Roofline Position
```
Performance (TFLOPS)
    â”‚
 65 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â† Peak Compute (Prefill)
    â”‚                             \
 10 â”œ                              \
    â”‚                               â— Easy Wins (AIâ‰ˆ20)
  1 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€/â”€â”€â”€â”€
    â”‚      Original              /
 0.1â”œâ”€â”€â”€â”€(AIâ‰ˆ3)â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€/â”€â”€â”€â”€â”€â”€
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€
              10             203
           AI (FLOP/Byte)
           
Easy Wins moved us higher on the memory-bound slope!
```

---

## Recommendations

### Production Configuration

**Use: `start_vllm_optimized.sh`**
```bash
#!/bin/bash
python -m vllm.entrypoints.openai.api_server \
  --model TheBloke/Llama-2-7B-AWQ \
  --dtype half \
  --max-model-len 2048 \
  --max-num-seqs 8 \\          # â† KEY OPTIMIZATION
  --gpu-memory-utilization 0.90 \
  --quantization awq \
  --enforce-eager \
  --port 8000
```

### When to Use Each Configuration

| Scenario | Recommended | Reason |
|----------|-------------|--------|
| Production (default) | Easy Wins | Best balance of throughput/latency |
| Latency-sensitive | Original | Fastest TTFT |
| Throughput-maximized | Easy Wins | +22% tokens/s |
| Long-context (RAG) | Easy Wins + chunked | Chunked helps with 1000+ tokens |
| Short chat | Easy Wins | Chunked doesn't help |

---

## Hardware Upgrade Path

### Current Limitations (T4)
- âŒ No FP8 support
- âŒ Limited memory bandwidth (320 GB/s)
- âŒ Older architecture (Turing)

### Recommended Upgrades

| GPU | Memory BW | FP8 | Est. Improvement | Cost |
|-----|-----------|-----|------------------|------|
| T4 (current) | 320 GB/s | âŒ | Baseline | - |
| L4 | 300 GB/s | âœ… | +20% efficiency | 2-3x |
| A10G | 600 GB/s | âŒ | +50-80% throughput | 2-3x |
| A100 | 1,555 GB/s | âŒ | +100-150% throughput | 4-5x |
| L40S | 864 GB/s | âœ… | +100% throughput | 3-4x |

---

## Key Learnings

### What Worked
1. âœ… **Batch size optimization** - Significant improvement (+22% throughput, -85% TPOT)
2. âœ… **Roofline analysis** - Accurately identified memory-bound bottleneck
3. âœ… **Measured trade-offs** - Higher TTFT but faster overall completion

### What Didn't Work
1. âŒ **Chunked prefill** - Added overhead without benefit for short inputs
2. âŒ **FP8 KV cache** - Hardware limitation (T4)
3. âŒ **Speculative decoding** - Requires draft model

### General Principles
1. ğŸ“Š **Measure everything** - Don't assume optimizations help
2. ğŸ¯ **Workload matters** - Same optimization, different results per workload
3. ğŸ”§ **Hardware constraints** - Know your GPU's capabilities
4. âš–ï¸ **Trade-offs exist** - Throughput vs latency is a real trade-off

---

## Files Reference

```
varcas/profiles/roofline/
â”‚
â”œâ”€â”€ start_vllm.sh                    # Original configuration
â”œâ”€â”€ start_vllm_optimized.sh          # âœ… RECOMMENDED (Easy Wins)
â”œâ”€â”€ start_vllm_advanced.sh           # Medium-term (chunked prefill)
â”‚
â”œâ”€â”€ OPTIMIZATION_PROGRESSION.md      # This document
â”œâ”€â”€ EASY_WINS_SUMMARY.md             # Easy wins results
â”œâ”€â”€ MEDIUM_TERM_RESULTS.md           # Medium-term results
â”œâ”€â”€ RESULTS_SUMMARY.md               # Roofline analysis
â”‚
â”œâ”€â”€ easy_wins_results/
â”‚   â”œâ”€â”€ baseline.json                # Original test results
â”‚   â””â”€â”€ optimized.json               # Easy wins test results
â”‚
â””â”€â”€ medium_term_results/
    â”œâ”€â”€ baseline.json                # Easy wins baseline
    â””â”€â”€ advanced.json                # Chunked prefill results
```

---

## Summary

**The Easy Wins optimization delivered significant improvements:**
- ğŸš€ **+22% token throughput**
- âš¡ **-85% decode latency** (6.7x faster)
- â±ï¸ **-35% total latency**

**Medium-term optimizations didn't help** for this specific workload due to hardware constraints and workload characteristics.

**Final Recommendation**: Use `start_vllm_optimized.sh` with `--max-num-seqs 8` for production deployment.

---

*Generated by roofline analysis and optimization tools*
