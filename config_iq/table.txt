=============================================================================================================================================================================================================================================================================================================================================================================================
Category            | Deployment / Use Case                                      | Model Type (Typical Size)      | Key Metrics & Acceptable Range (Target)                                                                 | Input Len | Output Len | Batch | Concurrency / QPS     | Notes / Context
--------------------|-----------------------------------------------------------|--------------------------------|---------------------------------------------------------------------------------------------------------|-----------|------------|-------|-----------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Real‑time Interact. | Conversational Chatbot (ChatGPT, support)                 | LLM 7B–70B                    | TTFT <200–300ms, TPOT <20–50ms, E2E <2–3s                                                              | 256–512   | 128–256    | 1     | Low (1–10 QPS/user)   | TTFT <200ms = "instant"; TPOT feels like fluent reading. p95 ≤2× avg.
                    | Code Completion (GitHub Copilot)                         | LLM 1B–7B                    | TTFT <50–100ms, TPOT <10–30ms, suggestion <150ms                                                       | 50–200    | 10–30      | 1     | Very low (bursty)     | Developer‑sensitive; needs small models or speculative decoding.
                    | Real‑time Translation / Transcription                    | LLM / Seq2Seq 0.5B–7B       | TTFT <300–500ms, TPOT <30–60ms, chunk <1s                                                              | 50–100    | 50–100     | 1–4   | Low                   | First token quick; later tokens can be slightly slower but fluid.
                    | Voice Assistants (end‑to‑end with ASR/TTS)               | LLM + ASR/TTS 7B–13B        | Overall <1–2s, LLM TPOT <40–80ms (TTFT <200ms)                                                         | 50–150    | 50–150     | 1     | Low                   | Users accept ~1s “thinking time”. LLM part must be fast.
                    | Interactive Search / RAG                                 | LLM + Retriever 7B–70B      | Retrieval <100–200ms, TTFT <200–300ms, TPOT <20–40ms                                                   | 50–200    | 100–300    | 1     | Low                   | Sub‑second total response expected.
Near Real‑time      | API‑Based Sync Inference (B2B, general)                  | Various 0.5B–70B            | p95 latency <500–1000ms, throughput contract‑dependent                                                 | 128–512   | 64–256     | 1–8   | Medium (10–100 QPS)   | p95 <500ms for medium models; very large models may allow <2s.
                    | Content Moderation                                       | Text classifiers / LLM 0.1B–7B | Latency per item <100–300ms                                                                           | 128–512   | N/A        | 1–32  | Medium                | Real‑time needs low latency; offline can batch.
                    | Document Summarization (interactive)                    | LLM 7B–70B                  | TTFT <500ms, TPOT <30–60ms, total <3–5s                                                               | 1024–2048 | 128–512    | 1     | Low                   | First token should appear quickly; total time proportional to output length.
                    | Embedding Generation                                     | BERT / sBERT 0.1B–1B       | Latency: CPU <10–30ms, GPU <1–5ms; throughput 100–1000+ vec/s                                          | 128–512   | N/A        | 32–256| High (throughput‑opt) | Batch‑size sensitive; usually throughput‑optimised.
Streaming & Persistent | Live Transcription / Captioning                       | Whisper, Conformer 0.1B–1.5B | First word <300–500ms, WER <5–10%, chunk <1s                                                          | audio (100–600 tok) | N/A | 1 | Low (per stream)      | Low latency per audio chunk; accuracy is also an SLA.
                    | Agentic Systems (multi‑step reasoning)                  | LLM + tools 7B–70B          | Step latency <1–2s, E2E <5–10s                                                                        | 100–400/step | 50–200/step | 1 | Low                   | Users tolerate longer pauses if reasoning is complex; each step responsive.
Offline / Batch     | Batch Inference (document processing, enrichment)       | Any 0.5B–70B               | Throughput max (tokens/s), cost per 1M tokens min                                                    | 512–2048   | 64–512     | 8–128 | Very high (async)     | Latency secondary; objective is lowest $/inference. Often runs on spot.
                    | Overnight Fine‑tuning / Training                        | Any                        | Time to completion <12–24h, cost budget‑dependent                                                    | N/A (training) | N/A | N/A | N/A                   | Not a serving SLA, but part of deployment pipeline.
Edge / On‑device    | Mobile Chat Assistant                                   | 1B–7B quantized (4/2‑bit)  | TTFT <50–100ms, TPOT <10–30ms, power <2–5W                                                           | 64–256    | 64–128     | 1     | Very low (single user)| Extreme constraints; aggressive quantization required.
                    | On‑device Code Completion                               | <1B quantized              | Suggestion latency <20–50ms                                                                           | 50–150    | 5–20       | 1     | Very low              | Must run without network; tiny, highly optimized models.
Vision & Multimodal | Real‑time Object Detection (auto, security)            | YOLO, ViT 10M–100M         | Latency <10–30ms, FPS ≥30–100                                                                         | N/A (image) | N/A | 1 | Deterministic         | Safety‑critical; deterministic low latency. Input e.g. 640×640.
                    | Image Classification (web)                              | ResNet, EfficientNet 10M–100M | Latency <50–100ms                                                                                    | N/A (image) | N/A | 1–32 | Medium                | Often batched; quick results needed.
                    | Image Generation (interactive)                         | Stable Diffusion, DALL·E 1B–5B | E2E latency <2–5s, throughput 0.2–0.5 images/s                                                       | text 50–100 | N/A | 1 | Low                   | Users expect a few seconds wait; longer for high‑res / multi‑image.
Specialized         | Text‑to‑Speech                                          | Tacotron, FastSpeech 0.1B–0.5B | RTF <0.1, first audio <200–300ms                                                                     | text 50–200 | N/A | 1 | Low                   | Must generate audio faster than it plays; low first‑chunk latency.
                    | Speech‑to‑Text                                          | Whisper, Conformer 0.1B–1.5B | RTF <0.1–0.2, WER <5–10%                                                                            | audio       | N/A (text) | 1 | Low                   | Similar to TTS; often domain‑optimised.
=============================================================================================================================================================================================================================================================================================================================================================================================
