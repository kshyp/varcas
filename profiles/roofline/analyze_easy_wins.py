#!/usr/bin/env python3
import json
from pathlib import Path

results_dir = Path("varcas/profiles/roofline/easy_wins_results")

# Load results
with open(results_dir / "baseline.json") as f:
    baseline = json.load(f)["metrics"]
with open(results_dir / "optimized.json") as f:
    optimized = json.load(f)["metrics"]

print("="*75)
print("EASY WINS OPTIMIZATION - RESULTS SUMMARY")
print("="*75)

print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                        CONFIGURATIONS COMPARED                           â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  BASELINE:  Original start_vllm.sh                                       â•‘
â•‘             â€¢ Default batching                                           â•‘
â•‘                                                                          â•‘
â•‘  OPTIMIZED: Easy Wins Applied                                           â•‘
â•‘             â€¢ max_num_seqs=8 (increased batching)                       â•‘
â•‘             â€¢ Better memory bandwidth utilization                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
print("â”‚ THROUGHPUT METRICS                                                      â”‚")
print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
tok_imp = ((optimized['throughput_tok_s']/baseline['throughput_tok_s']-1)*100)
req_imp = ((optimized['throughput_rps']/baseline['throughput_rps']-1)*100)
print(f"â”‚  Token Throughput:   {baseline['throughput_tok_s']:>7.1f} â†’ {optimized['throughput_tok_s']:<7.1f} tok/s   ({tok_imp:+.1f}%)  â”‚")
print(f"â”‚  Request Throughput: {baseline['throughput_rps']:>7.2f} â†’ {optimized['throughput_rps']:<7.2f} req/s   ({req_imp:+.1f}%)  â”‚")
print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

print("\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
print("â”‚ LATENCY METRICS                                                         â”‚")
print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
ttft_p50_imp = ((optimized['ttft_p50_ms']/baseline['ttft_p50_ms']-1)*100)
ttft_p99_imp = ((optimized['ttft_p99_ms']/baseline['ttft_p99_ms']-1)*100)
tpot_p50_imp = ((optimized['tpot_p50_ms']/baseline['tpot_p50_ms']-1)*100)
tpot_p99_imp = ((optimized['tpot_p99_ms']/baseline['tpot_p99_ms']-1)*100)
lat_p50_imp = ((optimized['latency_p50_ms']/baseline['latency_p50_ms']-1)*100)
lat_p99_imp = ((optimized['latency_p99_ms']/baseline['latency_p99_ms']-1)*100)

print(f"â”‚  TTFT p50:          {baseline['ttft_p50_ms']:>8.0f} â†’ {optimized['ttft_p50_ms']:<8.0f} ms    ({ttft_p50_imp:+.1f}%)  â”‚")
print(f"â”‚  TTFT p99:          {baseline['ttft_p99_ms']:>8.0f} â†’ {optimized['ttft_p99_ms']:<8.0f} ms    ({ttft_p99_imp:+.1f}%)  â”‚")
print(f"â”‚  TPOT p50:          {baseline['tpot_p50_ms']:>8.1f} â†’ {optimized['tpot_p50_ms']:<8.1f} ms    ({tpot_p50_imp:+.1f}%)  â”‚")
print(f"â”‚  TPOT p99:          {baseline['tpot_p99_ms']:>8.1f} â†’ {optimized['tpot_p99_ms']:<8.1f} ms    ({tpot_p99_imp:+.1f}%)  â”‚")
print(f"â”‚  Latency p50:       {baseline['latency_p50_ms']:>8.0f} â†’ {optimized['latency_p50_ms']:<8.0f} ms    ({lat_p50_imp:+.1f}%)  â”‚")
print(f"â”‚  Latency p99:       {baseline['latency_p99_ms']:>8.0f} â†’ {optimized['latency_p99_ms']:<8.0f} ms    ({lat_p99_imp:+.1f}%)  â”‚")
print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

# Generate interpretation
print("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
print("â•‘                         ANALYSIS & INSIGHTS                              â•‘")
print("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")

if tok_imp > 15:
    print(f"â•‘  âœ… TOKEN THROUGHPUT: +{tok_imp:.0f}% improvement!                              â•‘")
    print("â•‘     Batching allows better memory bandwidth utilization                 â•‘")
else:
    print(f"â•‘  â„¹ï¸  TOKEN THROUGHPUT: {tok_imp:+.1f}% change                                  â•‘")

if tpot_p50_imp < -50:
    print(f"â•‘  âœ… DECODE SPEED (TPOT): {tpot_p50_imp:.0f}% faster!                          â•‘")
    print("â•‘     Dramatic improvement in token generation speed                      â•‘")
    print("â•‘     (from ~230ms to ~36ms per token)                                    â•‘")

if ttft_p50_imp > 100:
    print(f"â•‘  âš ï¸  TIME TO FIRST TOKEN: +{ttft_p50_imp:.0f}% increase                        â•‘")
    print("â•‘     Higher due to queue wait with larger batches                        â•‘")
    print("â•‘     Trade-off: wait longer, but get tokens faster                       â•‘")

if lat_p50_imp < -20:
    print(f"â•‘  âœ… TOTAL LATENCY: {lat_p50_imp:.0f}% reduction                              â•‘")
    print("â•‘     Overall request completion is faster                                â•‘")

print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

print("\n" + "="*75)
print("VISUAL COMPARISON")
print("="*75)

# Visual bars for key metrics
max_tok = max(baseline['throughput_tok_s'], optimized['throughput_tok_s'])
baseline_tok_bar = int(baseline['throughput_tok_s'] / max_tok * 40)
optimized_tok_bar = int(optimized['throughput_tok_s'] / max_tok * 40)

print(f"\nToken Throughput (tok/s):")
print(f"  Baseline:  [{'â–ˆ' * baseline_tok_bar}{'â–‘' * (40-baseline_tok_bar)}] {baseline['throughput_tok_s']:.1f}")
print(f"  Optimized: [{'â–ˆ' * optimized_tok_bar}{'â–‘' * (40-optimized_tok_bar)}] {optimized['throughput_tok_s']:.1f}")

# TPOT (lower is better, so invert for visualization)
max_tpot = max(baseline['tpot_p50_ms'], optimized['tpot_p50_ms'])
baseline_tpot_bar = int((max_tpot - baseline['tpot_p50_ms']) / max_tpot * 40)
optimized_tpot_bar = int((max_tpot - optimized['tpot_p50_ms']) / max_tpot * 40)

print(f"\nDecode Speed (inverse TPOT, higher is better):")
print(f"  Baseline:  [{'â–ˆ' * baseline_tpot_bar}{'â–‘' * (40-baseline_tpot_bar)}] {baseline['tpot_p50_ms']:.1f} ms/tok")
print(f"  Optimized: [{'â–ˆ' * optimized_tpot_bar}{'â–‘' * (40-optimized_tpot_bar)}] {optimized['tpot_p50_ms']:.1f} ms/tok")

# Latency
max_lat = max(baseline['latency_p50_ms'], optimized['latency_p50_ms'])
baseline_lat_bar = int((max_lat - baseline['latency_p50_ms']) / max_lat * 40)
optimized_lat_bar = int((max_lat - optimized['latency_p50_ms']) / max_lat * 40)

print(f"\nRequest Completion Speed (inverse latency, higher is better):")
print(f"  Baseline:  [{'â–ˆ' * baseline_lat_bar}{'â–‘' * (40-baseline_lat_bar)}] {baseline['latency_p50_ms']:.0f} ms")
print(f"  Optimized: [{'â–ˆ' * optimized_lat_bar}{'â–‘' * (40-optimized_lat_bar)}] {optimized['latency_p50_ms']:.0f} ms")

print("\n" + "="*75)
print("OPTIMIZED CONFIGURATION (start_vllm_optimized.sh)")
print("="*75)
print("""#!/bin/bash
python -m vllm.entrypoints.openai.api_server \\
  --model TheBloke/Llama-2-7B-AWQ \\
  --dtype half \\
  --max-model-len 2048 \\
  --max-num-seqs 8 \\          # â† KEY: Increased batch size
  --gpu-memory-utilization 0.90 \\
  --quantization awq \\
  --enforce-eager \\
  --port 8000""")
print("="*75)

print("\nğŸ“ Results saved to: varcas/profiles/roofline/easy_wins_results/")
print("   â”œâ”€â”€ baseline.json")
print("   â””â”€â”€ optimized.json")

# Summary statistics
print("\n" + "="*75)
print("SUMMARY")
print("="*75)
print(f"""
âœ… Successfully applied "Easy Wins" optimizations:
   â€¢ Increased batch size (max_num_seqs=8)
   
ğŸ“Š Key Improvements:
   â€¢ Token Throughput: +{tok_imp:.0f}% ({baseline['throughput_tok_s']:.1f} â†’ {optimized['throughput_tok_s']:.1f} tok/s)
   â€¢ Decode Speed: {tpot_p50_imp:.0f}% faster ({baseline['tpot_p50_ms']:.0f} â†’ {optimized['tpot_p50_ms']:.0f} ms/token)
   â€¢ Total Latency: {lat_p50_imp:.0f}% reduction ({baseline['latency_p50_ms']:.0f} â†’ {optimized['latency_p50_ms']:.0f} ms)

âš ï¸  Trade-offs:
   â€¢ TTFT increased due to queue wait with larger batches
   â€¢ Overall: requests take longer to start but complete faster
""")
