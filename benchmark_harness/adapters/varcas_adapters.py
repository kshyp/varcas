"""
Varcas Load Adapters v1.0
Convert external benchmark formats to harness-compatible profiles.
"""

import json
import csv
import random
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime

from varcas_load_harness import (
    LoadProfile, WorkloadProfile, TokenDistribution,
    ArrivalProcess, WorkloadType, TokenDistribution
)


class MLPerfAdapter:
    """
    Convert MLPerf Inference scenarios to harness profiles.
    
    MLPerf scenarios:
    - SingleStream: One query at a time, measure latency
    - MultiStream: Fixed query rate, concurrent
    - Server: Poisson arrival, target latency bound
    - Offline: All queries at once, measure throughput
    """
    
    SCENARIOS = {
        "single_stream": {
            "arrival": ArrivalProcess.TRACE,
            "concurrency": 1,
            "open_loop": False,
            "description": "One request at a time, measure pure latency"
        },
        "multi_stream": {
            "arrival": ArrivalProcess.POISSON,
            "target_rps": 10.0,  # Configurable
            "open_loop": True,
            "description": "Fixed rate concurrent requests"
        },
        "server": {
            "arrival": ArrivalProcess.POISSON,
            "target_rps": 20.0,
            "open_loop": True,
            "description": "Poisson arrival, latency constraint"
        },
        "offline": {
            "arrival": ArrivalProcess.TRACE,
            "open_loop": True,
            "description": "Batch all requests immediately"
        }
    }
    
    @classmethod
    def from_dataset(cls, 
                     dataset_path: str,
                     scenario: str = "server",
                     target_latency_ms: float = 1000.0,
                     max_queries: int = 1000) -> LoadProfile:
        """
        Create MLPerf-style profile from dataset.
        
        dataset_path: JSON lines or CSV with 'input' and 'output' fields
        scenario: single_stream, multi_stream, server, offline
        """
        # Load dataset
        queries = cls._load_dataset(dataset_path, max_queries)
        
        config = cls.SCENARIOS.get(scenario, cls.SCENARIOS["server"])
        
        if scenario == "offline":
            # All queries at t=0
            trace_timestamps = [0.0] * len(queries)
        elif scenario == "single_stream":
            # Sequential: wait for each to complete
            trace_timestamps = list(range(len(queries)))  # Placeholder, harness handles sequencing
        else:
            trace_timestamps = None  # Generated by arrival process
        
        # Compute length distributions from dataset
        input_lengths = [q.get("input_tokens", len(q["input"]) // 4) for q in queries]
        output_lengths = [q.get("output_tokens", len(q["output"]) // 4) for q in queries]
        
        return LoadProfile(
            name=f"mlperf_{scenario}",
            arrival_process=config["arrival"],
            target_rps=config.get("target_rps", 10.0),
            concurrency=config.get("concurrency", 10),
            duration_seconds=300,  # MLPerf standard
            open_loop=config["open_loop"],
            trace_timestamps=trace_timestamps,
            workloads=[
                WorkloadProfile(
                    name=f"mlperf_{scenario}",
                    workload_type=WorkloadType.CHAT,
                    input_dist=TokenDistribution(
                        mean=sum(input_lengths) / len(input_lengths),
                        std=(max(input_lengths) - min(input_lengths)) / 4,
                        min=min(input_lengths),
                        max=max(input_lengths)
                    ),
                    output_dist=TokenDistribution(
                        mean=sum(output_lengths) / len(output_lengths),
                        std=(max(output_lengths) - min(output_lengths)) / 4,
                        min=min(output_lengths),
                        max=max(output_lengths)
                    )
                )
            ]
        )
    
    @classmethod
    def _load_dataset(cls, path: str, max_queries: int) -> List[Dict]:
        queries = []
        
        if path.endswith('.jsonl'):
            with open(path, 'r') as f:
                for i, line in enumerate(f):
                    if i >= max_queries:
                        break
                    queries.append(json.loads(line))
        
        elif path.endswith('.json'):
            with open(path, 'r') as f:
                data = json.load(f)
                queries = data[:max_queries]
        
        elif path.endswith('.csv'):
            with open(path, 'r') as f:
                reader = csv.DictReader(f)
                for i, row in enumerate(reader):
                    if i >= max_queries:
                        break
                    queries.append(row)
        
        return queries


class ShareGPTAdapter:
    """
    Convert ShareGPT dataset to harness profile.
    
    ShareGPT format: List of conversations with 'conversations' array
    Each conversation: [{'from': 'human', 'value': '...'}, {'from': 'gpt', 'value': '...'}]
    """
    
    @classmethod
    def from_file(cls,
                  sharegpt_path: str,
                  max_conversations: int = 1000,
                  turn_limit: int = 1) -> LoadProfile:
        """
        Convert ShareGPT to load profile.
        
        turn_limit: 1 = first turn only (Q&A), 2+ = multi-turn simulation
        """
        with open(sharegpt_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        conversations = data[:max_conversations]
        
        # Extract length statistics
        input_lengths = []
        output_lengths = []
        
        for conv in conversations:
            turns = conv.get("conversations", [])
            if not turns:
                continue
            
            # First human message
            for turn in turns:
                if turn.get("from") == "human":
                    text = turn.get("value", "")
                    input_lengths.append(len(text) // 4)
                    break
            
            # First assistant response
            for turn in turns:
                if turn.get("from") == "gpt":
                    text = turn.get("value", "")
                    output_lengths.append(len(text) // 4)
                    break
        
        if not input_lengths:
            input_lengths = [50]
        if not output_lengths:
            output_lengths = [100]
        
        return LoadProfile(
            name="sharegpt_replay",
            arrival_process=ArrivalProcess.POISSON,
            target_rps=10.0,  # Conservative default
            duration_seconds=120,
            open_loop=True,
            workloads=[
                WorkloadProfile(
                    name="sharegpt_chat",
                    workload_type=WorkloadType.CHAT,
                    input_dist=TokenDistribution(
                        mean=sum(input_lengths) / len(input_lengths),
                        std=statistics.stdev(input_lengths) if len(input_lengths) > 1 else 20,
                        min=min(input_lengths),
                        max=max(input_lengths)
                    ),
                    output_dist=TokenDistribution(
                        mean=sum(output_lengths) / len(output_lengths),
                        std=statistics.stdev(output_lengths) if len(output_lengths) > 1 else 50,
                        min=min(output_lengths),
                        max=max(output_lengths)
                    )
                )
            ]
        )
    
    @classmethod
    def to_ground_truth(cls, sharegpt_path: str, max_examples: int = 100) -> List[Dict]:
        """Extract ground truth examples for validation."""
        with open(sharegpt_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        examples = []
        for conv in data[:max_examples]:
            turns = conv.get("conversations", [])
            human_turn = None
            gpt_turn = None
            
            for turn in turns:
                if turn.get("from") == "human" and human_turn is None:
                    human_turn = turn.get("value", "")
                elif turn.get("from") == "gpt" and gpt_turn is None:
                    gpt_turn = turn.get("value", "")
                    break
            
            if human_turn and gpt_turn:
                examples.append({
                    "example_id": f"sharegpt:{len(examples)}",
                    "prompt": human_turn,
                    "reference": gpt_turn,
                    "validation_type": "chat"
                })
        
        return examples


class AzureTracesAdapter:
    """
    Convert Azure LLM Inference Traces to harness profile.
    
    Azure format (simplified):
    - Timestamp (ms or seconds)
    - Context tokens (input)
    - Generated tokens (output)
    - Optional: model name, region, etc.
    """
    
    @classmethod
    def from_file(cls,
                  trace_path: str,
                  time_unit: str = "ms",  # "ms", "s", or "datetime"
                  max_requests: int = 10000) -> LoadProfile:
        """
        Convert Azure trace to replay-able profile.
        """
        timestamps = []
        input_tokens = []
        output_tokens = []
        
        # Detect format
        if trace_path.endswith('.csv'):
            timestamps, input_tokens, output_tokens = cls._parse_csv(
                trace_path, time_unit, max_requests
            )
        elif trace_path.endswith('.json'):
            timestamps, input_tokens, output_tokens = cls._parse_json(
                trace_path, time_unit, max_requests
            )
        
        if not timestamps:
            raise ValueError("No valid trace data found")
        
        # Normalize timestamps to start at 0
        start_time = min(timestamps)
        trace_timestamps = [t - start_time for t in timestamps]
        
        # Determine workload type from length distribution
        mean_input = sum(input_tokens) / len(input_tokens)
        workload_type = WorkloadType.RAG if mean_input > 1000 else WorkloadType.CHAT
        
        return LoadProfile(
            name="azure_trace_replay",
            arrival_process=ArrivalProcess.TRACE,
            trace_timestamps=trace_timestamps,
            duration_seconds=int(max(trace_timestamps)) + 10,
            open_loop=True,
            workloads=[
                WorkloadProfile(
                    name="azure_workload",
                    workload_type=workload_type,
                    input_dist=TokenDistribution(
                        mean=mean_input,
                        std=statistics.stdev(input_tokens) if len(input_tokens) > 1 else mean_input * 0.2,
                        min=min(input_tokens),
                        max=max(input_tokens)
                    ),
                    output_dist=TokenDistribution(
                        mean=sum(output_tokens) / len(output_tokens),
                        std=statistics.stdev(output_tokens) if len(output_tokens) > 1 else 50,
                        min=min(output_tokens),
                        max=max(output_tokens)
                    )
                )
            ]
        )
    
    @classmethod
    def _parse_csv(cls, path: str, time_unit: str, max_rows: int) -> Tuple[List[float], List[int], List[int]]:
        timestamps = []
        input_tokens = []
        output_tokens = []
        
        with open(path, 'r') as f:
            reader = csv.DictReader(f)
            
            # Detect column names (Azure traces have varied schemas)
            sample = next(reader)
            f.seek(0)
            next(reader)  # Skip header again
            
            # Common column name variations
            time_cols = ['timestamp', 'Timestamp', 'time', 'Time', 'timestamp_ms']
            input_cols = ['context_tokens', 'input_tokens', 'prompt_tokens', 'ContextTokens']
            output_cols = ['generated_tokens', 'output_tokens', 'completion_tokens', 'GeneratedTokens']
            
            time_col = next((c for c in time_cols if c in sample), 'timestamp')
            input_col = next((c for c in input_cols if c in sample), 'context_tokens')
            output_col = next((c for c in output_cols if c in sample), 'generated_tokens')
            
            for i, row in enumerate(reader):
                if i >= max_rows:
                    break
                
                try:
                    ts = float(row[time_col])
                    if time_unit == "ms":
                        ts = ts / 1000.0  # Convert to seconds
                    
                    inp = int(row[input_col])
                    out = int(row[output_col])
                    
                    timestamps.append(ts)
                    input_tokens.append(inp)
                    output_tokens.append(out)
                    
                except (KeyError, ValueError):
                    continue
        
        return timestamps, input_tokens, output_tokens
    
    @classmethod
    def _parse_json(cls, path: str, time_unit: str, max_rows: int) -> Tuple[List[float], List[int], List[int]]:
        with open(path, 'r') as f:
            data = json.load(f)
        
        timestamps = []
        input_tokens = []
        output_tokens = []
        
        for entry in data[:max_rows]:
            try:
                ts = float(entry.get('timestamp', entry.get('Timestamp', 0)))
                if time_unit == "ms":
                    ts = ts / 1000.0
                
                inp = int(entry.get('context_tokens', entry.get('input_tokens', 0)))
                out = int(entry.get('generated_tokens', entry.get('output_tokens', 0)))
                
                timestamps.append(ts)
                input_tokens.append(inp)
                output_tokens.append(out)
                
            except (KeyError, ValueError):
                continue
        
        return timestamps, input_tokens, output_tokens


class vLLMBenchmarkAdapter:
    """
    Replicate vLLM's built-in benchmark behavior.
    
    vLLM benchmark_serving.py uses:
    - ShareGPT or synthetic dataset
    - Poisson arrival
    - Fixed request rate (target QPS)
    - Measures TTFT, TPOT, throughput
    """
    
    @classmethod
    def replicate(cls,
                  dataset_type: str = "sharegpt",  # "sharegpt" or "synthetic"
                  dataset_path: Optional[str] = None,
                  target_qps: float = 10.0,
                  num_prompts: int = 1000,
                  request_rate: Optional[float] = None) -> LoadProfile:
        """
        Create profile matching vLLM benchmark behavior.
        """
        if request_rate:
            target_qps = request_rate
        
        if dataset_type == "sharegpt" and dataset_path:
            # Use ShareGPT adapter then modify arrival
            profile = ShareGPTAdapter.from_file(dataset_path, num_prompts)
            profile.arrival_process = ArrivalProcess.POISSON
            profile.target_rps = target_qps
            profile.name = f"vllm_benchmark_sharegpt_qps{target_qps}"
            return profile
        
        else:
            # Synthetic: uniform random lengths
            return LoadProfile(
                name=f"vllm_benchmark_synthetic_qps{target_qps}",
                arrival_process=ArrivalProcess.POISSON,
                target_rps=target_qps,
                duration_seconds=num_prompts / target_qps,
                open_loop=True,
                workloads=[
                    WorkloadProfile(
                        name="vllm_synthetic",
                        workload_type=WorkloadType.CHAT,
                        input_dist=TokenDistribution(
                            mean=550, std=230, min=4, max=1024, distribution="uniform"
                        ),
                        output_dist=TokenDistribution(
                            mean=150, std=100, min=4, max=512, distribution="uniform"
                        )
                    )
                ]
            )


# ============================================================================
# UNIFIED LOADER
# ============================================================================

def load_external_format(
    file_path: str,
    format_type: str,  # "mlperf", "sharegpt", "azure", "vllm"
    **kwargs
) -> LoadProfile:
    """
    Universal loader for external benchmark formats.
    
    Examples:
        load_external_format("sharegpt.json", "sharegpt")
        load_external_format("azure_trace.csv", "azure", time_unit="s")
        load_external_format("dataset.json", "mlperf", scenario="server")
    """
    
    loaders = {
        "mlperf": MLPerfAdapter.from_dataset,
        "sharegpt": ShareGPTAdapter.from_file,
        "azure": AzureTracesAdapter.from_file,
        "vllm": vLLMBenchmarkAdapter.replicate
    }
    
    loader = loaders.get(format_type)
    if not loader:
        raise ValueError(f"Unknown format: {format_type}. Choose from {list(loaders.keys())}")
    
    return loader(file_path, **kwargs) if file_path else loader(**kwargs)


# ============================================================================
# CLI INTERFACE
# ============================================================================

if __name__ == "__main__":
    import argparse
    import statistics
    
    parser = argparse.ArgumentParser(description='Varcas Benchmark Adapters')
    parser.add_argument('--format', required=True, 
                       choices=['mlperf', 'sharegpt', 'azure', 'vllm'],
                       help='Source format')
    parser.add_argument('--input', help='Input file path')
    parser.add_argument('--output', default='profile.json', help='Output profile JSON')
    
    # MLPerf options
    parser.add_argument('--scenario', default='server',
                       choices=['single_stream', 'multi_stream', 'server', 'offline'],
                       help='MLPerf scenario')
    
    # Azure options
    parser.add_argument('--time-unit', default='ms', choices=['ms', 's'],
                       help='Timestamp unit in Azure trace')
    
    # vLLM options
    parser.add_argument('--target-qps', type=float, default=10.0,
                       help='Target queries per second')
    parser.add_argument('--num-prompts', type=int, default=1000,
                       help='Number of prompts')
    
    args = parser.parse_args()
    
    # Load based on format
    if args.format == "mlperf":
        profile = MLPerfAdapter.from_dataset(
            args.input, args.scenario
        )
    elif args.format == "sharegpt":
        profile = ShareGPTAdapter.from_file(args.input)
    elif args.format == "azure":
        profile = AzureTracesAdapter.from_file(
            args.input, args.time_unit
        )
    elif args.format == "vllm":
        profile = vLLMBenchmarkAdapter.replicate(
            dataset_path=args.input,
            target_qps=args.target_qps,
            num_prompts=args.num_prompts
        )
    
    # Save profile
    import json
    from dataclasses import asdict
    
    def serialize(obj):
        if isinstance(obj, ArrivalProcess):
            return obj.value
        if isinstance(obj, WorkloadType):
            return obj.value
        if isinstance(obj, TokenDistribution):
            return {
                "mean": obj.mean,
                "std": obj.std,
                "min": obj.min,
                "max": obj.max,
                "distribution": obj.distribution
            }
        raise TypeError(f"Object of type {type(obj)} is not JSON serializable")
    
    with open(args.output, 'w') as f:
        json.dump(asdict(profile), f, indent=2, default=serialize)
    
    print(f"Converted {args.format} to harness profile: {args.output}")
    print(f"Profile: {profile.name}")
    print(f"Arrival: {profile.arrival_process.value}")
    print(f"Duration: {profile.duration_seconds}s")
    print(f"Workloads: {len(profile.workloads)}")
